{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57472c3c-473f-4a35-8232-05718adf66d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c25488b-4d25-40cc-b96b-1d2b393641ec",
   "metadata": {},
   "source": [
    "# Импорт данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0b2b2ee-64b3-4eee-bed6-763e18e49a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тренировочные данные\n",
    "train_card_spending_df = pd.read_parquet('input_data/train_card_spending_df.parquet')\n",
    "train_main_df = pd.read_parquet('input_data/train_main_df.parquet')\n",
    "train_mcc_operations_df = pd.read_parquet('input_data/train_mcc_operations_df.parquet')\n",
    "train_mcc_preferences_df = pd.read_parquet('input_data/train_mcc_preferences_df.parquet')\n",
    "train_target = pd.read_csv('input_data/train_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c8f12-ab3a-4cda-8262-1730977d3651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тестовые данные\n",
    "test_card_spending_df = pd.read_parquet('input_data/test_card_spending_df.parquet')\n",
    "test_main_df = pd.read_parquet('input_data/test_main_df.parquet')\n",
    "test_mcc_operations_df = pd.read_parquet('input_data/test_mcc_operations_df.parquet')\n",
    "test_mcc_preferences_df = pd.read_parquet('input_data/test_mcc_preferences_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd1fce-12f5-49ab-991f-739077d470c8",
   "metadata": {},
   "source": [
    "# Очистка повторов id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cdf3fe2-bb3e-4447-b18c-fe0091103e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Т.к. порядок id похоже совпадает в train_target и train_main_df и других таблицах, продублируем index в отдельный столбец для merge\n",
    "# будем производить merge по двум колонкам для избежания новых фейковых данных\n",
    "train_target['index'] = train_target.index\n",
    "train_main_df['index'] = train_main_df.index\n",
    "train_card_spending_df['index'] = train_card_spending_df.index\n",
    "train_mcc_operations_df['index'] = train_mcc_operations_df.index\n",
    "train_mcc_preferences_df['index'] = train_mcc_preferences_df.index\n",
    "\n",
    "train_full_info = pd.merge(train_target, train_main_df, on=['id', 'index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266a226-83c2-43da-a101-5fcbc8998f7f",
   "metadata": {},
   "source": [
    "# Сценарий 1: Фантазирование всего"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ceb2ccea-d090-4868-ab9b-43a6edcf327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_8556\\1074550607.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_full_info['max_term_category'] = pd.cut(\n",
      "C:\\Temp\\ipykernel_8556\\1074550607.py:49: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  median_values = train_full_info.groupby('max_term_category')['cnt_prolong_max'].median()\n"
     ]
    }
   ],
   "source": [
    "def all_nans_fullfilling(train_full_info):\n",
    "    # заполнение nan в колонке происхождения авто и степени БУ авто\n",
    "    train_full_info['vehicle_counrty_type_nm'] = train_full_info['vehicle_counrty_type_nm'].fillna(0)\n",
    "    train_full_info['used_car_flg'] = train_full_info['used_car_flg'].fillna(0)\n",
    "    train_full_info['app_vehicle_ind'] = train_full_info['app_vehicle_ind'].fillna(0)\n",
    "    \n",
    "    \n",
    "    # идея заполнения однотипных колонок по маске\n",
    "    mask_avg_dep = train_full_info.columns.str.startswith('avg_dep_avg_balance_fact_')\n",
    "    mask_zp = train_full_info.columns.str.startswith('zp_')\n",
    "    mask_agg = train_full_info.columns.str.startswith(('max_', 'min_', 'avg_', 'sum_'))\n",
    "    mask_dep = train_full_info.columns.str.startswith('dep_')\n",
    "    mask_income = train_full_info.columns.str.startswith('income_')\n",
    "    mask_cnt = train_full_info.columns.str.startswith('cnt_') & ~train_full_info.columns.isin(['cnt_prolong_max', 'cnt_prolong_max_5y'])\n",
    "    # заполнение медианой колонок из списка avg_dep_avg_balance_fact_...\n",
    "    cols_to_fill = train_full_info.columns[mask_avg_dep]\n",
    "    train_full_info[cols_to_fill] = train_full_info[cols_to_fill].fillna(train_full_info[cols_to_fill].median())\n",
    "    # заполнение медианой колонок из списка zp_...\n",
    "    cols_to_fill = train_full_info.columns[mask_zp]\n",
    "    train_full_info[cols_to_fill] = train_full_info[cols_to_fill].fillna(train_full_info[cols_to_fill].median())\n",
    "    # заполнение медианой колонок из списка max_..., min_..., avg_..., sum_...\n",
    "    cols_to_fill = train_full_info.columns[mask_agg]\n",
    "    train_full_info[cols_to_fill] = train_full_info[cols_to_fill].fillna(train_full_info[cols_to_fill].median())\n",
    "    # заполнение медианой колонок из списка dep_...\n",
    "    cols_to_fill = train_full_info.columns[mask_dep]\n",
    "    train_full_info[cols_to_fill] = train_full_info[cols_to_fill].fillna(train_full_info[cols_to_fill].median())\n",
    "    # заполнение медианой колонок из списка income_...\n",
    "    cols_to_fill = train_full_info.columns[mask_income]\n",
    "    train_full_info[cols_to_fill] = train_full_info[cols_to_fill].fillna(train_full_info[cols_to_fill].median())\n",
    "    # заполнение медианой колонок из списка cnt_...\n",
    "    cols_to_fill = train_full_info.columns[mask_cnt]\n",
    "    train_full_info[cols_to_fill] = train_full_info[cols_to_fill].fillna(train_full_info[cols_to_fill].median())\n",
    "    \n",
    "    \n",
    "    # вывод: требуется написание функции, для cnt_prolong_max которая бы ставила 0, 1, 2 на основании значения\n",
    "    # dep_max_d_term (макс срок срочного вклада) и max_term (макс срок договора) для cnt_prolong_max\n",
    "    # А cnt_prolong_max_5y обработать на основании cnt_prolong_max\n",
    "    # Определяем границы интервалов и соответствующие метки\n",
    "    bins = [0, 7, 12, 31, 32, 33, 41, 51, 58, 61, 63, 75, 80, float('inf')]\n",
    "    labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "    # Создаем категории для max_term\n",
    "    train_full_info['max_term_category'] = pd.cut(\n",
    "        train_full_info['max_term'],\n",
    "        bins=bins,\n",
    "        labels=labels,\n",
    "        right=False\n",
    "    )\n",
    "    # Вычисляем медианные значения cnt_prolong_max для каждой категории\n",
    "    median_values = train_full_info.groupby('max_term_category')['cnt_prolong_max'].median()\n",
    "    # Заполняем пропуски в cnt_prolong_max на основе категорий max_term\n",
    "    train_full_info['cnt_prolong_max'] = train_full_info['cnt_prolong_max'].fillna(\n",
    "        train_full_info['max_term_category'].map(median_values))\n",
    "    # Удаляем временную колонку (опционально)\n",
    "    train_full_info = train_full_info.drop('max_term_category', axis=1)\n",
    "    train_full_info['cnt_prolong_max_5y'] = train_full_info['cnt_prolong_max_5y'].fillna(0)\n",
    "    \n",
    "    \n",
    "    # Вывод: app_income_app = mean(income_verified, income_verified_primary_job) + income_unverified\n",
    "    func = lambda x: (x['income_verified'] + x['income_verified_primary_job']) / 2 + x['income_unverified']\n",
    "    train_full_info['app_income_app'] = train_full_info.apply(func, axis=1)\n",
    "    \n",
    "    # Заполнение app_real_estate_ind (наличие недвижимости)\n",
    "    train_full_info['app_real_estate_ind'] = train_full_info['app_real_estate_ind'].fillna('0')\n",
    "    \n",
    "    \n",
    "    # Вывод: попробуем заполнить nan в детях и иждвенцах нулями. nan в семье возьмем как сумма детей и иждивенцев\n",
    "    train_full_info['app_children_cnt'] = train_full_info['app_children_cnt'].fillna(0)\n",
    "    train_full_info['app_dependent_cnt'] = train_full_info['app_dependent_cnt'].fillna(0)\n",
    "    filling = train_full_info['app_children_cnt'] + train_full_info['app_dependent_cnt']\n",
    "    train_full_info['app_family_cnt'] = train_full_info['app_family_cnt'].fillna(filling)\n",
    "\n",
    "    return train_full_info\n",
    "\n",
    "train_full_info = all_nans_fullfilling(train_full_info)\n",
    "\n",
    "\n",
    "#Лучший ответ: 1.9891\n",
    "    #CATBOOST:\n",
    "    #iterations=2500,\n",
    "    #learning_rate=0.05,\n",
    "    #loss_function='RMSE',\n",
    "    #eval_metric='R2',\n",
    "    #random_seed=42,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58639fc8-cf44-4b96-8f36-6d787bb2b121",
   "metadata": {},
   "source": [
    "# Сценарий 2: Удаление всего, где много nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "16a3cbb3-f92c-4e57-9ee5-6d10b2651297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleting_nan_columns(train_full_info):\n",
    "    # заполнение медианой колонок из списка cnt_...\n",
    "    mask_cnt = train_full_info.columns.str.startswith('cnt_') & ~train_full_info.columns.isin(['cnt_prolong_max', 'cnt_prolong_max_5y'])\n",
    "    cols_to_fill = train_full_info.columns[mask_cnt]\n",
    "    train_full_info[cols_to_fill] = train_full_info[cols_to_fill].fillna(train_full_info[cols_to_fill].median())\n",
    "    \n",
    "    mask_avg_dep = train_full_info.columns.str.startswith('avg_dep_avg_balance_fact_')\n",
    "    mask_zp = train_full_info.columns.str.startswith('zp_')\n",
    "    mask_agg = train_full_info.columns.str.startswith(('max_', 'min_', 'avg_', 'sum_'))\n",
    "    mask_dep = train_full_info.columns.str.startswith('dep_')\n",
    "    mask_income = train_full_info.columns.str.startswith('income_')\n",
    "    mask_cnt = train_full_info.columns.str.startswith('cnt_')\n",
    "    mask_app = train_full_info.columns.str.startswith('app_')\n",
    "    \n",
    "    \n",
    "    # Создаем список всех колонок для удаления\n",
    "    cols_to_drop = (train_full_info.columns[mask_avg_dep].tolist() +\n",
    "                    train_full_info.columns[mask_zp].tolist() +\n",
    "                    train_full_info.columns[mask_agg].tolist() +\n",
    "                    train_full_info.columns[mask_dep].tolist() +\n",
    "                    train_full_info.columns[mask_income].tolist() +\n",
    "                    train_full_info.columns[mask_cnt].tolist() +\n",
    "                    train_full_info.columns[mask_app].tolist()\n",
    "                   )\n",
    "    # Удаляем все колонки за один раз\n",
    "    train_full_info = train_full_info.drop(columns=cols_to_drop)\n",
    "\n",
    "    train_full_info = train_full_info.drop(['vehicle_counrty_type_nm'], axis=1)\n",
    "    \n",
    "    return train_full_info\n",
    "\n",
    "train_full_info = deleting_nan_columns(train_full_info)\n",
    "\n",
    "\n",
    "#Лучший ответ: 2.1095\n",
    "    #CATBOOST:\n",
    "    #iterations=1000,\n",
    "    #learning_rate=0.05,\n",
    "    #loss_function='RMSE',\n",
    "    #eval_metric='R2',\n",
    "    #random_seed=42,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5cb61c-4d78-4911-aab8-7d25375aa1d7",
   "metadata": {},
   "source": [
    "# Сценарий 3: Совмещение сценария 1 и 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3cba3300-0302-4178-87c6-fbeb17a0a706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_8556\\1145654557.py:46: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  median_values = train_full_info.groupby('max_term_category')['cnt_prolong_max'].median()\n"
     ]
    }
   ],
   "source": [
    "def deleting_and_filling(train_full_info):\n",
    "    train_full_info['vehicle_counrty_type_nm'] = train_full_info['vehicle_counrty_type_nm'].fillna(0)\n",
    "    train_full_info['used_car_flg'] = train_full_info['used_car_flg'].fillna(0)\n",
    "    train_full_info['app_vehicle_ind'] = train_full_info['app_vehicle_ind'].fillna(0)\n",
    "    train_full_info['app_real_estate_ind'] = train_full_info['app_real_estate_ind'].fillna('0')\n",
    "\n",
    "    # заполнение медианой колонок из списка cnt_... и maxterm\n",
    "    mask_cnt = train_full_info.columns.str.startswith('cnt_') & ~train_full_info.columns.isin(['cnt_prolong_max', 'cnt_prolong_max_5y'])\n",
    "    cols_to_fill = train_full_info.columns[mask_cnt]\n",
    "    train_full_info[cols_to_fill] = train_full_info[cols_to_fill].fillna(train_full_info[cols_to_fill].median())\n",
    "    train_full_info['max_term'] = train_full_info['max_term'].fillna(train_full_info['max_term'].median())\n",
    "    # заполнение медианой колонок из списка income_...\n",
    "    mask_income = train_full_info.columns.str.startswith('income_')\n",
    "    cols_to_fill = train_full_info.columns[mask_income]\n",
    "    train_full_info[cols_to_fill] = train_full_info[cols_to_fill].fillna(train_full_info[cols_to_fill].median())\n",
    "    \n",
    "    # удаление колонок\n",
    "    mask_avg_dep = train_full_info.columns.str.startswith('avg_dep_avg_balance_fact_')\n",
    "    mask_zp = train_full_info.columns.str.startswith('zp_')\n",
    "    mask_agg = (train_full_info.columns.str.startswith(('max_', 'min_', 'avg_', 'sum_'))\n",
    "                & ~train_full_info.columns.isin(['max_term']))\n",
    "    mask_dep = train_full_info.columns.str.startswith('dep_')\n",
    "    #mask_cnt = train_full_info.columns.str.startswith('cnt_')\n",
    "    # Создаем список всех колонок для удаления\n",
    "    cols_to_drop = (train_full_info.columns[mask_avg_dep].tolist() +\n",
    "                    train_full_info.columns[mask_zp].tolist() +\n",
    "                    train_full_info.columns[mask_agg].tolist() +\n",
    "                    train_full_info.columns[mask_dep].tolist()\n",
    "                    #train_full_info.columns[mask_cnt].tolist()\n",
    "                    )\n",
    "    # Удаляем все колонки за один раз\n",
    "    train_full_info = train_full_info.drop(columns=cols_to_drop)\n",
    "    train_full_info = train_full_info.drop(['app_children_cnt', 'app_dependent_cnt', 'app_family_cnt', 'cnt_prolong_max_5y'], axis=1)\n",
    "\n",
    "    # Определяем границы интервалов и соответствующие метки\n",
    "    bins = [0, 7, 12, 31, 32, 33, 41, 51, 58, 61, 63, 75, 80, float('inf')]\n",
    "    labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "    # Создаем категории для max_term\n",
    "    train_full_info['max_term_category'] = pd.cut(\n",
    "        train_full_info['max_term'],\n",
    "        bins=bins,\n",
    "        labels=labels,\n",
    "        right=False\n",
    "    )\n",
    "    # Вычисляем медианные значения cnt_prolong_max для каждой категории\n",
    "    median_values = train_full_info.groupby('max_term_category')['cnt_prolong_max'].median()\n",
    "    # Заполняем пропуски в cnt_prolong_max на основе категорий max_term\n",
    "    train_full_info['cnt_prolong_max'] = train_full_info['cnt_prolong_max'].fillna(\n",
    "        train_full_info['max_term_category'].map(median_values))\n",
    "    # Удаляем временную колонку (опционально)\n",
    "    train_full_info = train_full_info.drop('max_term_category', axis=1)\n",
    "    \n",
    "    # Вывод: app_income_app = mean(income_verified, income_verified_primary_job) + income_unverified\n",
    "    func = lambda x: (x['income_verified'] + x['income_verified_primary_job']) / 2 + x['income_unverified']\n",
    "    train_full_info['app_income_app'] = train_full_info.apply(func, axis=1)\n",
    "    \n",
    "    return train_full_info\n",
    "\n",
    "train_full_info = deleting_and_filling(train_full_info)\n",
    "\n",
    "#Лучший ответ: 2.1090\n",
    "    #CATBOOST:\n",
    "    #iterations=1000,\n",
    "    #learning_rate=0.05,\n",
    "    #loss_function='RMSE',\n",
    "    #eval_metric='R2',\n",
    "    #random_seed=42,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8d027-f66d-4a1a-9a21-62049c0dc1a5",
   "metadata": {},
   "source": [
    "# Сценарий 4-5. Очистка вспомогательных таблиц (nan более x%) и присоединение их к основной из сценария 1 или 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e816d88c-cd40-4e8a-933b-11f71125b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ПРЕЖДЕ ВЫПОЛНИТЬ СЦЕНАРИЙ 1 ИЛИ 2\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Удаляем колонки, где NaN > x\n",
    "    x = 0.5\n",
    "    threshold = len(df) * 0.75\n",
    "    cols_to_drop = df.columns[df.isnull().sum() > threshold]\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Создаем словарь для группировки колонок по базовым названиям\n",
    "    from collections import defaultdict\n",
    "    col_groups = defaultdict(list)\n",
    "    \n",
    "    # Группируем колонки по базовым названиям (без временных суффиксов)\n",
    "    for col in df.columns:\n",
    "        base_col = col\n",
    "        for suffix in ['_1m', '_3m', '_6m', '_12m', '_1', '_3', '_6', '_12', '_7d']:\n",
    "            if col.endswith(suffix):\n",
    "                base_col = col[:-len(suffix)]\n",
    "                break\n",
    "        col_groups[base_col].append(col)\n",
    "    \n",
    "    # Выбираем какие колонки оставить (приоритет: _1m, затем _1, затем _12m/_12)\n",
    "    cols_to_keep = set()\n",
    "    cols_to_remove = set()\n",
    "    \n",
    "    for base_col, variants in col_groups.items():\n",
    "        if len(variants) > 1:  # Если есть временные варианты\n",
    "            # Проверяем наличие предпочтительных вариантов\n",
    "            preferred = [v for v in variants if v.endswith(('_1m', '_1'))]\n",
    "            if not preferred:\n",
    "                preferred = [v for v in variants if v.endswith(('_12m', '_12'))]\n",
    "            \n",
    "            if preferred:\n",
    "                # Оставляем первый предпочтительный вариант\n",
    "                cols_to_keep.add(preferred[0])\n",
    "                # Остальные добавляем на удаление\n",
    "                for v in variants:\n",
    "                    if v != preferred[0]:\n",
    "                        cols_to_remove.add(v)\n",
    "    \n",
    "    # Удаляем колонки с другими периодами\n",
    "    df = df.drop(columns=cols_to_remove)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Обработка каждой таблицы\n",
    "train_card_spending_df = preprocess_data(train_card_spending_df)\n",
    "train_mcc_operations_df = preprocess_data(train_mcc_operations_df)\n",
    "train_mcc_preferences_df = preprocess_data(train_mcc_preferences_df)\n",
    "\n",
    "# Обработка каждой таблицы\n",
    "train_card_spending_df = preprocess_data(train_card_spending_df)\n",
    "train_mcc_operations_df = preprocess_data(train_mcc_operations_df)\n",
    "train_mcc_preferences_df = preprocess_data(train_mcc_preferences_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "36016e9d-6dd4-4e10-a1e0-0b5e7e3206c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавить для сценария 4\n",
    "train_full_info = pd.merge(train_full_info, train_card_spending_df, on=['id', 'index'])\n",
    "train_full_info = pd.merge(train_full_info, train_mcc_operations_df, on=['id', 'index'])\n",
    "train_full_info = pd.merge(train_full_info, train_mcc_preferences_df, on=['id', 'index'])\n",
    "\n",
    "#Лучший ответ: 1.9990\n",
    "    # Сценарий 1 + очистка nan у вспомогательных таблиц - более 75% и более 50%\n",
    "    #iterations=2500,\n",
    "    #learning_rate=0.05,\n",
    "    #loss_function='RMSE',\n",
    "    #eval_metric='R2',\n",
    "    #random_seed=42,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75d632-222e-47a9-918c-a99fcd8c4a57",
   "metadata": {},
   "source": [
    "# Сценарий 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80111713-f77c-42e6-9958-e4c738c0e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_card_spending_df = pd.merge(train_target, train_card_spending_df, on=['id', 'index'])\n",
    "train_mcc_operations_df = pd.merge(train_target, train_mcc_operations_df, on=['id', 'index'])\n",
    "train_mcc_preferences_df = pd.merge(train_target, train_mcc_preferences_df, on=['id', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de19ee-c042-4a13-8d44-313e4f0e3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_card_spending_df.select_dtypes(include=['number']).corr()\n",
    "corr_matrix_dea = corr_matrix.loc[['target'], :]\n",
    "corr_matrix_dea.loc[:, (corr_matrix_dea.abs() > 0.4).any(axis = 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7625cad-aa86-417c-be63-65d7894384a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_correlation(df, target_col='target', correlation_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Фильтрует колонки датафрейма по корреляции с целевой переменной,\n",
    "    затем удаляет дублирующие временные колонки.\n",
    "    \n",
    "    Параметры:\n",
    "    - df: исходный датафрейм\n",
    "    - target_col: название целевой колонки\n",
    "    - correlation_threshold: порог корреляции для отбора колонок\n",
    "    \n",
    "    Возвращает:\n",
    "    - Отфильтрованный датафрейм\n",
    "    \"\"\"\n",
    "    # Проверяем, что целевая колонка существует\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Целевая колонка '{target_col}' не найдена в датафрейме\")\n",
    "    \n",
    "    # Вычисляем корреляцию только для числовых колонок\n",
    "    numeric_df = df.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Получаем матрицу корреляций с целевой колонкой\n",
    "    corr_matrix = numeric_df.corr()\n",
    "    target_corr = corr_matrix.loc[[target_col], :]\n",
    "    \n",
    "    # Выбираем колонки с корреляцией выше порога (по модулю)\n",
    "    significant_cols = target_corr.columns[(target_corr.abs() > correlation_threshold).any(axis=0)]\n",
    "    \n",
    "    # Оставляем только значимые колонки + саму целевую\n",
    "    filtered_df = df[list(significant_cols) + [target_col]]\n",
    "    \n",
    "    # Теперь применяем обработку похожих колонок (как в предыдущем решении)\n",
    "    def remove_temporal_duplicates(temp_df):\n",
    "        \"\"\"Вспомогательная функция для удаления временных дубликатов\"\"\"\n",
    "        from collections import defaultdict\n",
    "        col_groups = defaultdict(list)\n",
    "        \n",
    "        # Группируем колонки по базовым названиям\n",
    "        for col in temp_df.columns:\n",
    "            if col == target_col:\n",
    "                continue  # Целевую колонку не трогаем\n",
    "                \n",
    "            base_col = col\n",
    "            for suffix in ['_1m', '_3m', '_6m', '_12m', '_1', '_3', '_6', '_12', '_7d']:\n",
    "                if col.endswith(suffix):\n",
    "                    base_col = col[:-len(suffix)]\n",
    "                    break\n",
    "            col_groups[base_col].append(col)\n",
    "        \n",
    "        cols_to_keep = set()\n",
    "        cols_to_remove = set()\n",
    "        \n",
    "        for base_col, variants in col_groups.items():\n",
    "            if len(variants) > 1:  # Если есть временные варианты\n",
    "                # Приоритет: _1m, затем _1, затем _12m/_12\n",
    "                preferred = [v for v in variants if v.endswith(('_1m', '_1'))] or \\\n",
    "                           [v for v in variants if v.endswith(('_12m', '_12'))]\n",
    "                \n",
    "                if preferred:\n",
    "                    cols_to_keep.add(preferred[0])\n",
    "                    for v in variants:\n",
    "                        if v != preferred[0]:\n",
    "                            cols_to_remove.add(v)\n",
    "        \n",
    "        # Удаляем дубликаты, оставляя целевую колонку\n",
    "        return temp_df.drop(columns=cols_to_remove)\n",
    "    \n",
    "    # Применяем функцию удаления дубликатов\n",
    "    final_df = remove_temporal_duplicates(filtered_df)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Пример использования:\n",
    "# processed_df = filter_by_correlation(train_full_info, target_col='target', correlation_threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fa92f16-1656-4709-93fc-e35df810794a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Temp\\ipykernel_9568\\1029122404.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdataset_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_full_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_card_spending_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mcc_operations_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mcc_preferences_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_by_correlation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Temp\\ipykernel_9568\\452482181.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(df, target_col, correlation_threshold)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Вычисляем корреляцию только для числовых колонок\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mnumeric_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'number'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Получаем матрицу корреляций с целевой колонкой\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mcorr_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumeric_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mtarget_corr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorr_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# Выбираем колонки с корреляцией выше порога (по модулю)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\.Distrib\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[0;32m  11048\u001b[0m         \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11049\u001b[0m         \u001b[0mmat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11051\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"pearson\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 11052\u001b[1;33m             \u001b[0mcorrel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnancorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  11053\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"spearman\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11054\u001b[0m             \u001b[0mcorrel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnancorr_spearman\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11055\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"kendall\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_list = [train_full_info, train_card_spending_df, train_mcc_operations_df, train_mcc_preferences_df]\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    dataset = filter_by_correlation(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b956d0-1376-4698-bc19-961cf2c3650a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ed71c8a-c90c-4194-9772-297ef4e3f186",
   "metadata": {},
   "source": [
    "# Проверка на заполнение пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a231fa08-00a7-4fef-8d6b-6d4d9b3e6736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "used_car_flg                        31299\n",
       "cat_maxspend_v1_1                   48985\n",
       "cat_maxspend_v2_1                   49348\n",
       "cc_avg_trns_1m                      26877\n",
       "cc_cnt_trns_12m                     26877\n",
       "cc_sum_trns_12m                     26877\n",
       "cnt_mcc_codes_1                     42301\n",
       "cnt_tr_abroad_1                     42301\n",
       "cnt_tr_airlines_1                   42301\n",
       "cnt_tr_alkochol_1m                  42301\n",
       "cnt_tr_all_1                        40416\n",
       "cnt_tr_appliance_retail_1           42301\n",
       "cnt_tr_auto_rental_1                42301\n",
       "cnt_tr_auto_repair_1m               42301\n",
       "cnt_tr_auto_services_1              42301\n",
       "cnt_tr_books_store_1                42301\n",
       "cnt_tr_business_services_1          42301\n",
       "cnt_tr_buy_1m                       42301\n",
       "cnt_tr_cash_1                       42301\n",
       "cnt_tr_cash_in_1                    40416\n",
       "cnt_tr_cash_services_1              42301\n",
       "cnt_tr_charity_1                    42301\n",
       "cnt_tr_cleaning_services_1          42301\n",
       "cnt_tr_computer_program_retail_1    42301\n",
       "cnt_tr_construction_services_1      42301\n",
       "cnt_tr_direct_marketing_1           42301\n",
       "cnt_tr_duty_free_1                  42301\n",
       "cnt_tr_education_1                  42301\n",
       "cnt_tr_entertainment_1              42301\n",
       "cnt_tr_fashion_retail_1             42301\n",
       "dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверка на отсуствие nan\n",
    "nan_count = train_full_info.isna().sum()\n",
    "nan_count = nan_count[nan_count != 0]\n",
    "nan_count.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abb4c89-7e76-4853-8000-e7f15c571ed3",
   "metadata": {},
   "source": [
    "# Обработка категориальных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ba184-ad88-4a3e-8c8e-ccd2064aa837",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_info.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cefc4e-69ed-4e55-bc67-ca5f2c50a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_info['savings_service_model_cd'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fa1191-329d-4792-b75b-4b0dd25336ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим две наиболее разносторонние категориальные колонки\n",
    "train_full_info = train_full_info.drop(['brand_nm', 'industry_nm'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00def35-c4ab-4405-8f69-da71cb8e5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = train_full_info.select_dtypes(include=['object']).columns\n",
    "\n",
    "train_encoded = pd.get_dummies(\n",
    "    train_full_info,\n",
    "    columns=cat_columns,\n",
    "    prefix=cat_columns,\n",
    "    drop_first=True  # избегаем дамми-ловушку\n",
    ").astype('float64')\n",
    "\n",
    "test_encoded = pd.get_dummies(\n",
    "    #train_full_info,\n",
    "    columns=cat_columns,\n",
    "    prefix=cat_columns,\n",
    "    drop_first=True  # избегаем дамми-ловушку\n",
    ").astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75780a20-ce29-4dab-ad9f-ecc3d855bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d6120-c4f0-469e-99ca-47129225d002",
   "metadata": {},
   "source": [
    "# Экспорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a693a300-ef47-4dc1-af5a-82d7015e72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_info.to_csv('processed_data.csv', index=False)\n",
    "\n",
    "#test_full_info.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17396d19-d2ff-41d5-84a7-4b8651bc81e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
